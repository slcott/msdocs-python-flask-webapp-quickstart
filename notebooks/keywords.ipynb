{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23e6ff00-a21b-44ec-a9ef-c1434a61d6a2",
   "metadata": {},
   "source": [
    "# Title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93174a9-b2e1-44c0-a88c-6aad91211a2d",
   "metadata": {},
   "source": [
    "## Import Difficulties\n",
    "Not sure why but install and import is inconcistent. There are two ways and some packages can only be used by install one of the ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28dc1827-bce3-4ec4-95ef-fe27445802df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rake-nltk\n",
      "  Using cached rake_nltk-1.0.6-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting nltk<4.0.0,>=3.6.2 (from rake-nltk)\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in /opt/homebrew/lib/python3.13/site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (8.1.7)\n",
      "Collecting joblib (from nltk<4.0.0,>=3.6.2->rake-nltk)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/homebrew/lib/python3.13/site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (2024.11.6)\n",
      "Collecting tqdm (from nltk<4.0.0,>=3.6.2->rake-nltk)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Using cached rake_nltk-1.0.6-py3-none-any.whl (9.1 kB)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, joblib, nltk, rake-nltk\n",
      "Successfully installed joblib-1.4.2 nltk-3.9.1 rake-nltk-1.0.6 tqdm-4.67.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install rake-nltk  --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "13ab3e57-61a7-4d0c-b43c-6954dd754d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package            Version\n",
      "------------------ -----------\n",
      "certifi            2024.8.30\n",
      "charset-normalizer 3.4.0\n",
      "click              8.1.7\n",
      "idna               3.10\n",
      "jellyfish          1.1.2\n",
      "joblib             1.4.2\n",
      "networkx           3.4.2\n",
      "nltk               3.9.1\n",
      "numpy              2.2.0\n",
      "pandas             2.2.3\n",
      "pip                24.3.1\n",
      "python-dateutil    2.9.0.post0\n",
      "pytz               2024.2\n",
      "rake-nltk          1.0.6\n",
      "regex              2024.11.6\n",
      "requests           2.32.3\n",
      "segtok             1.5.11\n",
      "six                1.17.0\n",
      "tabulate           0.9.0\n",
      "tqdm               4.67.1\n",
      "tzdata             2024.2\n",
      "urllib3            2.2.3\n",
      "yake               0.4.8\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c9bffcf6-6b88-4883-b6e1-4e7339a792da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rake_nltk import Rake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3cf130a7-51b9-4b5c-97b3-df6f6d0d2140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rake_nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13b10447-8902-4c6e-bdac-c1b23655fabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests)\n",
      "  Using cached charset_normalizer-3.4.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests)\n",
      "  Using cached urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests)\n",
      "  Using cached certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Using cached charset_normalizer-3.4.0-cp313-cp313-macosx_11_0_arm64.whl (119 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Installing collected packages: urllib3, idna, charset-normalizer, certifi, requests\n",
      "Successfully installed certifi-2024.8.30 charset-normalizer-3.4.0 idna-3.10 requests-2.32.3 urllib3-2.2.3\n"
     ]
    }
   ],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97aee01a-ec8c-40fd-beca-adc4ef863d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2edc1cb0-7676-433d-992c-edbf928f20b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (89 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/homebrew/lib/python3.13/site-packages (from pandas) (2.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Using cached pandas-2.2.3-cp313-cp313-macosx_11_0_arm64.whl (11.3 MB)\n",
      "Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Using cached tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.2.3 pytz-2024.2 tzdata-2024.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas  --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89a189cd-5a55-4d44-a764-3d38dcad9c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b7d86a91-8514-475d-89fc-1d193f89dc02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in ./my-env/lib/python3.13/site-packages (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11139261-45e9-42b3-a0d2-8957d130daf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b199d3a3-e8e4-465e-9d53-2fafd722dccf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de6ca1d6-15d6-4027-ace5-0fde24bdb196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting yake\n",
      "  Using cached yake-0.4.8-py2.py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting tabulate (from yake)\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting click>=6.0 (from yake)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.13/site-packages (from yake) (2.2.0)\n",
      "Collecting segtok (from yake)\n",
      "  Using cached segtok-1.5.11-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting networkx (from yake)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jellyfish (from yake)\n",
      "  Using cached jellyfish-1.1.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (2.6 kB)\n",
      "Collecting regex (from segtok->yake)\n",
      "  Using cached regex-2024.11.6-cp313-cp313-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Using cached yake-0.4.8-py2.py3-none-any.whl (60 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached jellyfish-1.1.2-cp313-cp313-macosx_11_0_arm64.whl (310 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Using cached segtok-1.5.11-py3-none-any.whl (24 kB)\n",
      "Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Using cached regex-2024.11.6-cp313-cp313-macosx_11_0_arm64.whl (284 kB)\n",
      "Installing collected packages: tabulate, regex, networkx, jellyfish, click, segtok, yake\n",
      "Successfully installed click-8.1.7 jellyfish-1.1.2 networkx-3.4.2 regex-2024.11.6 segtok-1.5.11 tabulate-0.9.0 yake-0.4.8\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install yake --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "472726d8-a003-49e5-b56d-261abac0c621",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96b7553-d877-4bd0-bf33-290f72e8b812",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4d5da931-15c4-4d8b-be1f-a6ab9ba1ea9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Sources tell us that Google is acquiring Kaggle, a platform that hosts data science and machine learning \"\\\n",
    "\"competitions. Details about the transaction remain somewhat vague, but given that Google is hosting its Cloud \"\\\n",
    "\"Next conference in San Francisco this week, the official announcement could come as early as tomorrow. \"\\\n",
    "\"Reached by phone, Kaggle co-founder CEO Anthony Goldbloom declined to deny that the acquisition is happening. \"\\\n",
    "\"Google itself declined 'to comment on rumors'. Kaggle, which has about half a million data scientists on its platform, \"\\\n",
    "\"was founded by Goldbloom  and Ben Hamner in 2010. \"\\\n",
    "\"The service got an early start and even though it has a few competitors like DrivenData, TopCoder and HackerRank, \"\\\n",
    "\"it has managed to stay well ahead of them by focusing on its specific niche. \"\\\n",
    "\"The service is basically the de facto home for running data science and machine learning competitions. \"\\\n",
    "\"With Kaggle, Google is buying one of the largest and most active communities for data scientists - and with that, \"\\\n",
    "\"it will get increased mindshare in this community, too (though it already has plenty of that thanks to Tensorflow \"\\\n",
    "\"and other projects). Kaggle has a bit of a history with Google, too, but that's pretty recent. Earlier this month, \"\\\n",
    "\"Google and Kaggle teamed up to host a $100,000 machine learning competition around classifying YouTube videos. \"\\\n",
    "\"That competition had some deep integrations with the Google Cloud Platform, too. Our understanding is that Google \"\\\n",
    "\"will keep the service running - likely under its current name. While the acquisition is probably more about \"\\\n",
    "\"Kaggle's community than technology, Kaggle did build some interesting tools for hosting its competition \"\\\n",
    "\"and 'kernels', too. On Kaggle, kernels are basically the source code for analyzing data sets and developers can \"\\\n",
    "\"share this code on the platform (the company previously called them 'scripts'). \"\\\n",
    "\"Like similar competition-centric sites, Kaggle also runs a job board, too. It's unclear what Google will do with \"\\\n",
    "\"that part of the service. According to Crunchbase, Kaggle raised $12.5 million (though PitchBook says it's $12.75) \"\\\n",
    "\"since its   launch in 2010. Investors in Kaggle include Index Ventures, SV Angel, Max Levchin, Naval Ravikant, \"\\\n",
    "\"Google chief economist Hal Varian, Khosla Ventures and Yuri Milner \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b7da74-f61a-4292-a7a7-841ffb4348d8",
   "metadata": {},
   "source": [
    "## Try using Rake NLTK\n",
    "\n",
    "https://csurfer.github.io/rake-nltk/_build/html/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b506a090-d4c7-4383-9efb-b1b9a7ff20f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/slcott/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/slcott/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rake_nltk import Rake\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c603a276-e65e-471b-bfaa-158be6cec4af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(52.583333333333336,\n",
       "  '000 machine learning competition around classifying youtube videos'),\n",
       " (21.6, 'google chief economist hal varian'),\n",
       " (21.0, 'founder ceo anthony goldbloom declined'),\n",
       " (16.0, 'transaction remain somewhat vague'),\n",
       " (16.0, 'official announcement could come'),\n",
       " (12.75, 'kaggle include index ventures'),\n",
       " (12.333333333333334, 'machine learning competitions'),\n",
       " (12.333333333333334, 'machine learning competitions'),\n",
       " (9.25, 'like similar competition'),\n",
       " (9.0, 'stay well ahead'),\n",
       " (9.0, 'sources tell us'),\n",
       " (9.0, 'get increased mindshare'),\n",
       " (9.0, 'de facto home'),\n",
       " (9.0, 'competitors like drivendata'),\n",
       " (9.0, 'company previously called'),\n",
       " (9.0, 'cloud next conference'),\n",
       " (8.8, 'hosts data science'),\n",
       " (8.8, 'analyzing data sets'),\n",
       " (8.3, 'running data science'),\n",
       " (8.0, 'though pitchbook says'),\n",
       " (7.8, 'million data scientists'),\n",
       " (7.75, 'kaggle also runs'),\n",
       " (6.1, 'google cloud platform'),\n",
       " (5.3, 'data scientists'),\n",
       " (5.0, 'khosla ventures'),\n",
       " (4.5, '5 million'),\n",
       " (4.0, 'yuri milner'),\n",
       " (4.0, 'sv angel'),\n",
       " (4.0, 'specific niche'),\n",
       " (4.0, 'service running'),\n",
       " (4.0, \"scripts ')\"),\n",
       " (4.0, 'san francisco'),\n",
       " (4.0, 'pretty recent'),\n",
       " (4.0, 'naval ravikant'),\n",
       " (4.0, 'max levchin'),\n",
       " (4.0, 'job board'),\n",
       " (4.0, 'interesting tools'),\n",
       " (4.0, 'even though'),\n",
       " (4.0, 'deep integrations'),\n",
       " (4.0, 'current name'),\n",
       " (4.0, 'centric sites'),\n",
       " (4.0, 'buying one'),\n",
       " (4.0, 'ben hamner'),\n",
       " (4.0, 'active communities'),\n",
       " (3.75, 'kaggle teamed'),\n",
       " (3.75, 'kaggle raised'),\n",
       " (3.75, 'kaggle co'),\n",
       " (3.75, 'acquiring kaggle'),\n",
       " (3.5, 'source code'),\n",
       " (3.5, 'service got'),\n",
       " (3.5, \"kernels ',\"),\n",
       " (3.5, 'early start'),\n",
       " (3.25, 'competition'),\n",
       " (3.25, 'competition'),\n",
       " (3.0, 'goldbloom'),\n",
       " (3.0, 'declined'),\n",
       " (2.0, 'though'),\n",
       " (1.75, 'kaggle'),\n",
       " (1.75, 'kaggle'),\n",
       " (1.75, 'kaggle'),\n",
       " (1.75, 'kaggle'),\n",
       " (1.75, 'kaggle'),\n",
       " (1.75, 'kaggle'),\n",
       " (1.6, 'google'),\n",
       " (1.6, 'google'),\n",
       " (1.6, 'google'),\n",
       " (1.6, 'google'),\n",
       " (1.6, 'google'),\n",
       " (1.6, 'google'),\n",
       " (1.6, 'google'),\n",
       " (1.6, 'google'),\n",
       " (1.5, 'service'),\n",
       " (1.5, 'service'),\n",
       " (1.5, 'platform'),\n",
       " (1.5, 'platform'),\n",
       " (1.5, 'platform'),\n",
       " (1.5, 'kernels'),\n",
       " (1.5, 'early'),\n",
       " (1.5, 'code'),\n",
       " (1.0, 'week'),\n",
       " (1.0, 'understanding'),\n",
       " (1.0, 'unclear'),\n",
       " (1.0, 'topcoder'),\n",
       " (1.0, 'tomorrow'),\n",
       " (1.0, 'thanks'),\n",
       " (1.0, 'tensorflow'),\n",
       " (1.0, 'technology'),\n",
       " (1.0, 'since'),\n",
       " (1.0, 'share'),\n",
       " (1.0, 'rumors'),\n",
       " (1.0, 'reached'),\n",
       " (1.0, 'projects'),\n",
       " (1.0, 'probably'),\n",
       " (1.0, 'plenty'),\n",
       " (1.0, 'phone'),\n",
       " (1.0, 'part'),\n",
       " (1.0, 'month'),\n",
       " (1.0, 'managed'),\n",
       " (1.0, 'likely'),\n",
       " (1.0, 'launch'),\n",
       " (1.0, 'largest'),\n",
       " (1.0, 'keep'),\n",
       " (1.0, 'investors'),\n",
       " (1.0, 'hosting'),\n",
       " (1.0, 'hosting'),\n",
       " (1.0, 'host'),\n",
       " (1.0, 'history'),\n",
       " (1.0, 'happening'),\n",
       " (1.0, 'half'),\n",
       " (1.0, 'hackerrank'),\n",
       " (1.0, 'given'),\n",
       " (1.0, 'founded'),\n",
       " (1.0, 'focusing'),\n",
       " (1.0, 'earlier'),\n",
       " (1.0, 'developers'),\n",
       " (1.0, 'details'),\n",
       " (1.0, 'deny'),\n",
       " (1.0, 'crunchbase'),\n",
       " (1.0, 'community'),\n",
       " (1.0, 'community'),\n",
       " (1.0, 'comment'),\n",
       " (1.0, 'build'),\n",
       " (1.0, 'bit'),\n",
       " (1.0, 'basically'),\n",
       " (1.0, 'basically'),\n",
       " (1.0, 'already'),\n",
       " (1.0, 'acquisition'),\n",
       " (1.0, 'acquisition'),\n",
       " (1.0, 'according'),\n",
       " (1.0, '75'),\n",
       " (1.0, '2010'),\n",
       " (1.0, '2010'),\n",
       " (1.0, '12'),\n",
       " (1.0, '12'),\n",
       " (1.0, '100')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Uses stopwords for english from NLTK, and all puntuation characters by\n",
    "# default\n",
    "r = Rake()\n",
    "\n",
    "# Extraction given the text.\n",
    "r.extract_keywords_from_text(text)\n",
    "\n",
    "sentences = text.split('.')\n",
    "\n",
    "# Extraction given the list of strings where each string is a sentence.\n",
    "r.extract_keywords_from_sentences(sentences)\n",
    "\n",
    "# To get keyword phrases ranked highest to lowest.\n",
    "r.get_ranked_phrases()\n",
    "\n",
    "# To get keyword phrases ranked highest to lowest with scores.\n",
    "r.get_ranked_phrases_with_scores()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4233c27c-9292-4a85-83af-ba7a744a2d9d",
   "metadata": {},
   "source": [
    "## Try Using Yake\n",
    "\n",
    "https://github.com/LIAAD/yake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fadcb239-52d2-481d-a73b-b5152a43bcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b0f75010-e568-4cff-86df-64fe21081871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Google', np.float64(0.026580863364597897))\n",
      "('Kaggle', np.float64(0.0289005976239829))\n",
      "('CEO Anthony Goldbloom', np.float64(0.029946071606210194))\n",
      "('San Francisco', np.float64(0.048810837074825336))\n",
      "('Anthony Goldbloom declined', np.float64(0.06176910090701819))\n",
      "('Google Cloud Platform', np.float64(0.06261974476422487))\n",
      "('co-founder CEO Anthony', np.float64(0.07357749587020043))\n",
      "('acquiring Kaggle', np.float64(0.08723571551039863))\n",
      "('CEO Anthony', np.float64(0.08915156857226395))\n",
      "('Anthony Goldbloom', np.float64(0.09123482372372106))\n",
      "('machine learning', np.float64(0.09147989238151344))\n",
      "('Kaggle co-founder CEO', np.float64(0.093805063905847))\n",
      "('data', np.float64(0.097574333771058))\n",
      "('Google Cloud', np.float64(0.10260128641464673))\n",
      "('machine learning competitions', np.float64(0.10773000650607861))\n",
      "('Francisco this week', np.float64(0.11519915079240485))\n",
      "('platform', np.float64(0.1183512305596321))\n",
      "('conference in San', np.float64(0.12392066376108138))\n",
      "('service', np.float64(0.12546743261462942))\n",
      "('Goldbloom', np.float64(0.14611408778815776))\n"
     ]
    }
   ],
   "source": [
    "kw_extractor = yake.KeywordExtractor()\n",
    "keywords = kw_extractor.extract_keywords(text)\n",
    "\n",
    "for kw in keywords:\n",
    "\tprint(kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "761daf01-1e6e-455b-8a2d-c0715a229621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Google', np.float64(0.026580863364597897))\n",
      "('Kaggle', np.float64(0.0289005976239829))\n",
      "('CEO Anthony Goldbloom', np.float64(0.029946071606210194))\n",
      "('San Francisco', np.float64(0.048810837074825336))\n",
      "('Anthony Goldbloom declined', np.float64(0.06176910090701819))\n",
      "('Google Cloud Platform', np.float64(0.06261974476422487))\n",
      "('co-founder CEO Anthony', np.float64(0.07357749587020043))\n",
      "('acquiring Kaggle', np.float64(0.08723571551039863))\n",
      "('CEO Anthony', np.float64(0.08915156857226395))\n",
      "('Anthony Goldbloom', np.float64(0.09123482372372106))\n",
      "('machine learning', np.float64(0.09147989238151344))\n",
      "('Kaggle co-founder CEO', np.float64(0.093805063905847))\n",
      "('data', np.float64(0.097574333771058))\n",
      "('Google Cloud', np.float64(0.10260128641464673))\n",
      "('machine learning competitions', np.float64(0.10773000650607861))\n",
      "('Francisco this week', np.float64(0.11519915079240485))\n",
      "('platform', np.float64(0.1183512305596321))\n",
      "('conference in San', np.float64(0.12392066376108138))\n",
      "('service', np.float64(0.12546743261462942))\n",
      "('Goldbloom', np.float64(0.14611408778815776))\n"
     ]
    }
   ],
   "source": [
    "language = \"en\"\n",
    "max_ngram_size = 3\n",
    "deduplication_threshold = 0.9\n",
    "deduplication_algo = 'seqm'\n",
    "windowSize = 1\n",
    "numOfKeywords = 20\n",
    "\n",
    "custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, dedupFunc=deduplication_algo, windowsSize=windowSize, top=numOfKeywords, features=None)\n",
    "keywords = custom_kw_extractor.extract_keywords(text)\n",
    "\n",
    "for kw in keywords:\n",
    "    print(kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b30c9424-019e-43a2-a1db-ec9a538acec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a b c', 'd e f', 'g h']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def chunk(text):\n",
    "    lo = 0\n",
    "    for hi in [ i.start() for i in re.finditer('\\\\.', text) ]:\n",
    "        yield text[lo:hi].strip()\n",
    "        lo = hi + 1\n",
    "        \n",
    "text = 'a b c. d e f. g h. i'\n",
    "print([x for x in chunk(text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79be214d-0651-4d4c-9b04-2f7c3bc86b4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env",
   "language": "python",
   "name": "my-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
